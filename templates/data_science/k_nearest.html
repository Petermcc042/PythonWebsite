{% extends "Essential_Templates/base_tutorial.html" %}
{% block left_column %}    
<a href="{{ url_for('pw_setup') }}"><b> Portfolio Website</b> </a>
<a href="#header">Title</a>
<a href="#overview">Overview</a>
<a href="#project setup">Setup</a>
<a href="#python instalation" style="font-size: 16px;"> - Python Instalation</a>
<a href="#virtenv note" style="font-size: 16px;"> - Virtual Environment</a>
<a href="#idle instalation" style="font-size: 16px;"> - IDLE</a>
<a href="#decisiontree">K-Nearest-Neighbours</a>
<a href="#background" style="font-size: 16px;"> - Background</a>
<a href="#algo-overview" style="font-size: 16px;"> - Algorithm Overview</a>
<a href="#evaluate-algorithm" style="font-size: 16px;"> - Evaluate Algorithm</a>
<a href="#cross-validation" style="font-size: 16px;"> - Cross Validation</a>
<a href="#k-nearest" style="font-size: 16px;"> - K-Nearest</a>
<a href="#predict" style="font-size: 16px;"> - Predict Classification</a>
<a href="#get-neighbours" style="font-size: 16px;"> - Get Neighbours</a>
<a href="#end">The End / Full Code</a>
{% endblock left_column %}   
      
{% block middle_column %}   
<div class="main base-tutorial">
  <h1><strong>Coding a K-Nearest Algorithm From Scratch</strong></h1>
  <h4>Peter McClintock</h4>
  <h4>24/10/2021</h4>
  <h1>Overview</h1>
  <ul>
    <li>The goal of this tutorial is to introduce the main features of coding the k-nearest neighbours algorithm</li>
    <li>The language used in this project is Python</li>
    <li>Other key skills are working with python in the command line</li>
    <li>The code will be introduced in order of run time with the full code file included at the end of the page.</li>
  </ul>
  <h1>Setup</h1>
  <h2>Python Instalation</h2>
  <p>
    To check if you have Python already installed open command prompt and type python or py and hit enter. The build version of python should return if installed.
    The version of python that this tutorial is built on is python 3.9.7 and hopefully future updates will not wreck any of the tutorial.
    If you do not have python installed a quick google search should direct you to the main website to download it. For the tutorial we are using the Windows Installer
    (64-bit) version.
  </p>
  <h2>Virtual Environment</h2>
  <p>
    A virtual environment acts as a separate version of python, libraries and scripts installed into it are isolated from any libraries installed in a “system” Python,
     i.e., one which is installed as part of your operating system.
  </p>
  <p>
    As is good practice we will be setting up virtual environment for this project. The virtual environment package can be installed through the command line using
    <code>py -m pip install virtualenv</code>. Using command line again we set the path to whichever folder you are completing this project in. You can navigate to your 
    project folder using <code>C:\your_standard_file_path> cd folder-name\folder-name\project-folder-name</code> . In my case this is 
    <code>C:\users\pmcc> cd documents\projects\decisiontree</code>. Now you can run the code <code>py -m venv virtual</code> to setup the virtual environment folder.
  </p>
  <p>Once python has been succesfully installed we can install pandas.</p>
  <p>To install any packages in the virtual environment we can use the command below subsituting package-name with our desired package, i.e. pandas.</p>
  <pre><code>C:\your_standard_file_path\decisiontree> virtual\Scripts\pip install package-name</code></pre>
  <p>To check that packages have been installed you can check what packages are in the environment using the command below. Make sure pandas is installed</p>
  <pre><code>C:\your_standard_file_path\decisiontree> virtual\Scripts\pip freeze</code></pre>
  <h2>IDLE</h2>
  <p>
    For this tutorial I have used IDLE as my IDE for simplicity. This is because with modern instalations of python IDLE downloads with it meaning you should now already 
    have it on your system! We will be running the IDE using our new virtual environment which can be achieved with a simple command. Back again to the command prompt we 
    now acitvate our virtual environment using <code>C:\your_standard_file_path\pandas-basics> virtual\Scripts\activate</code>. Then to open the idle input the command below.
  </p>
  <pre><code>(virtual) C:\your_standard_file_path\decisiontree> py -m idlelib.idle</code></pre>
  <p>This will open the IDLE Shell from which we can create a new file with (File > New File). Save this in your project folder</p>
  <p>
    Great now the project is set-up we can get to learning the basics!
  </p>
  <h1>K-Nearest Neighbours</h1>
  <h2>Background</h2>
  <p>
    The k-nearest-neighbours algorithm despite its simplicity can be quite succesful in classification problems. The algorithm essentially groups together data points based 
    on a criteria. This criteria is usually the Eucludean distance from other points. When predicting the k-most similar data points are used to make the prediction. As data 
    is taken from other points, the k-nearest-neighbour method can be used for classification and regression.
  </p>
  <h2>The Algorithm</h2>
  <p>Below is the overall process of the algorithm.</p>
  <ul>
    <li>Perform K-Fold Cross Validation</li>
    <li>For each fold run the decision tree algorithm</li>
    <li>Build the the tree using tree depth and size parameters</li>
    <li>Data is split using the Gini Index</li>
    <li>Calculate the accuracy score for each fold and print that to the terminal</li>
  </ul>
  <p>As such the sections from now will roughly follow the flow of the tree. Each section will break down the methods within the overall class</p>
  <h2>Evaluate Algorithm</h2>
  <p>
    The only command needed to run everything is <code>evaluate_algorithm()</code> which runs the subsequent methods in the class. It starts by implementing 
    cross validation on the dataset taking the n_folds parameter to determine how many folds are created. We then intialise a list to store the accuracy scores and begin a 
    for loop for each fold created. The for loop creates the standard cross validation procedure of testing a different individual subset of the data on each iteration.
  </p>
  <pre><code class="language-python" style="font-size: 20px;">
    # Evaluate an algorithm using a cross validation split
    def evaluate_algorithm(self):
        folds = self.cross_validation_split(self.dataset, self.n_folds)
        scores = list()
        for fold in folds:
            train_set = list(folds)
            train_set.remove(fold)
            train_set = sum(train_set, [])
            test_set = list()
            for row in fold:
                row_copy = list(row)
                test_set.append(row_copy)
                row_copy[-1] = None
            predicted = self.k_nearest_neighbours(train_set, test_set, self.num_neighbours)
            actual = [row[-1] for row in fold]
            accuracy = self.accuracy_metric(actual, predicted)
            scores.append(accuracy)
        print('Scores: %s' % scores)
        print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))
        return scores
  </code></pre>
  <p>
    After the training and test sets are split the algorithm runs <code>k_nearest_neighbours()</code> using the split data which returns an array of predicted values from the 
    algorithm. Finally, the accuracy value is calculated for each fold and returned in the scores list that we initialised. 
    <code>Accuracy = (Number of Correct Predictions)/Total Number of Predictions</code>
  </p>
  <p>The algorithm is designed in this initial stage that cross validation can be implemented regardless of what algorithm is used. One would simply need to add 
    different methods to apply a separate machine learning model and call it at the <code>k_nearest_neighbours()</code> stage.
  </p>
  <h2>K Fold Cross Validation</h2>
  <p>
    Attempting to reduce bias in the testing I have used K fold cross validation. This involves splitting the training data into multiple different sets in order 
    to train the model using different variations of the data. The general procedure is as follows:
  </p>
  <ul>
    <li>Shuffle the dataset randomly.</li>
    <li>Split the dataset into k groups</li>
    <li>For each unique group:</li>
    <li>Take the group as a hold out or test data set</li>
    <li>Take the remaining groups as a training data set</li>
    <li>Fit a model on the training set and evaluate it on the test set</li>
    <li>Retain the evaluation score and discard the model</li>
    <li>Summarize the skill of the model using the sample of model evaluation scores</li>
  </ul>
  <pre><code class="language-python" style="font-size: 20px;">
    def cross_validation_split(self, dataset, n_folds):
      dataset_split = list()
      dataset_copy = list(dataset)
      fold_size = int(len(dataset) / n_folds) # the row size of each of the splits
      for i in range(n_folds): # numbers 0 to nfolds -1
          fold = list() # this will store the new rearranged data
          while len(fold) < fold_size: 
              index = randrange(len(dataset_copy)) # gives a random number inside the dataset
              fold.append(dataset_copy.pop(index)) # appends a row using the  random number above 
          dataset_split.append(fold) # adds the fold to the dataset split list and repeats for another fold
      return dataset_split # produces the new split data set inside one list
  </code></pre>
  <p>
    The code takes the full dataset, randomly rearranges it, and splits it into folds based on the user input. It then stores the folds in a list named 
    dataset_split. As explained in the <code>evaluate_algorithm()</code> method, the fitting the model on each fold occurs there.
  </p>
  <h2>K-Nearest-Neighbours</h2>
  <p></p>
  <pre><code class="language-python" style="font-size: 20px;">
    def k_nearest_neighbours(self, train, test, num_neighbours):
      predictions = list()
      for row in test:
          output = self.predict_classification(train, row, num_neighbours)
          predictions.append(output)
      return(predictions)
  </code></pre>
  <p>
    We initialise a list to store the predictions then loop through each row in the test set and create a prediction using the <code>predict_classification()</code> 
    method. This is then added to the predictions list and we return our list at the end of the method.
  </p>
  <h2>Predict Classification</h2>
  <pre><code class="language-python" style="font-size: 20px;">
    def predict_classification(self, train, test_row, num_neighbours):
      neighbours = self.get_neighbours(train, test_row, num_neighbours)
      output_values = [row[-1] for row in neighbours]
      prediction = max(set(output_values), key=output_values.count)
      return prediction
  </code></pre>
  <p>
    <code>predict_classification()</code> takes in the training data, the data used for prediction and the number of neighbours that it should search for. The 
    <code>get_neighbours()</code> method returns the nearest data points to the row of data currenlty being assessed. It then stores this data in the neigbors variable. 
    A list of the class values of these points is stored and we return the most common class value in the neighbours data. The method ends by returning the most common 
    class value.
  </p>
  <h2>Get Neighbours</h2>
  <p>The next two methods provide the main math behind the algorithm</p>
  <pre><code class="language-python" style="font-size: 20px;">
    def get_neighbours(self, train, test_row, num_neighbours):
      distances = list()
      for train_row in train:
          dist = self.euclidean_distance(test_row, train_row)
          distances.append((train_row, dist))
      distances.sort(key=lambda tup: tup[1])
      neighbours = list()
      for i in range(num_neighbours):
          neighbours.append(distances[i][0])
      return neighbours

    def euclidean_distance(self, row1, row2):
      distance = 0.0
      for i in range(len(row1)-1):
          distance += (row1[i] - row2[i])**2
      return sqrt(distance)
  </code></pre>
  <p>
    Firstly we initialise a list to store distances. Then for every row in the training data set the Eucludean distance to the data you are testing is calculated. 
    The Euclidean distance can be calculated by passing in each row to the formula meaning it scales with feature size. Finally, the nearest neighbours are selected 
    based on the user input value and returned in a list.
  </p>
  <h1>The End</h1>
  <p>Congratulations if you made it to the end of the tutorial and have managed to run the model on some of your own data! If you haven't managed to run the code or are 
    encountering errors a full page of my code is included below with an example that should work. Good luch in your future projects.
  </p>
  <pre><code class="language-python">
    from random import seed
    from random import randrange
    import pandas as pd
    from math import sqrt


    class mykn:
        def __init__(self, dataset, n_folds, num_neighbours):
            self.dataset = dataset
            self.n_folds = n_folds
            self.num_neighbours = num_neighbours

        # Split a dataset into k folds
        def cross_validation_split(self, dataset, n_folds):
            dataset_split = list() 
            dataset_copy = list(dataset)
            fold_size = int(len(dataset) / n_folds) # the row size of each of the splits
            for i in range(n_folds): # numbers 0 to nfolds -1
                fold = list() # this will store the new rearranged data
                while len(fold) < fold_size: 
                    index = randrange(len(dataset_copy)) # gives a random number inside the dataset
                    fold.append(dataset_copy.pop(index)) # appends a row using the  random number above 
                dataset_split.append(fold) # adds the fold to the dataset split list and repeats for another fold
            return dataset_split # produces the new split data set inside one list
        
        # Calculate accuracy percentage
        def accuracy_metric(self, actual, predicted):
            correct = 0
            for i in range(len(actual)):
                if actual[i] == predicted[i]:
                    correct += 1
            return correct / float(len(actual)) * 100.0
        
        # Evaluate an algorithm using a cross validation split
        def evaluate_algorithm(self):
            folds = self.cross_validation_split(self.dataset, self.n_folds)
            scores = list()
            for fold in folds:
                train_set = list(folds)
                train_set.remove(fold)
                train_set = sum(train_set, [])
                test_set = list()
                for row in fold:
                    row_copy = list(row)
                    test_set.append(row_copy)
                    row_copy[-1] = None
                predicted = self.k_nearest_neighbours(train_set, test_set, self.num_neighbours)
                actual = [row[-1] for row in fold]
                accuracy = self.accuracy_metric(actual, predicted)
                scores.append(accuracy)
            return scores
        
        # Calculate the Euclidean distance between two vectors
        def euclidean_distance(self, row1, row2):
            distance = 0.0
            for i in range(len(row1)-1):
                distance += (row1[i] - row2[i])**2
            return sqrt(distance)

        # Locate the most similar neighbours
        def get_neighbours(self, train, test_row, num_neighbours):
            distances = list()
            for train_row in train:
                dist = self.euclidean_distance(test_row, train_row)
                distances.append((train_row, dist))
            distances.sort(key=lambda tup: tup[1])
            neighbours = list()
            for i in range(num_neighbours):
                neighbours.append(distances[i][0])
            return neighbours

        # Make a prediction with neighbours
        def predict_classification(self, train, test_row, num_neighbours):
            neighbours = self.get_neighbours(train, test_row, num_neighbours)
            output_values = [row[-1] for row in neighbours]
            prediction = max(set(output_values), key=output_values.count)
            return prediction
        
        # kNN Algorithm
        def k_nearest_neighbours(self, train, test, num_neighbours):
            predictions = list()
            for row in test:
                output = self.predict_classification(train, row, num_neighbours)
                predictions.append(output)
            return(predictions)
            


    # Test CART on Bank Note dataset
    seed(1)

    # load and prepare data
    filename = "https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt"
    df = pd.read_csv(filename, sep=',')
    dataset = df.values.tolist()

    # evaluate algorithm
    n_folds = 5
    num_neighbours = 5

    scores = mykn(dataset, n_folds, num_neighbours).evaluate_algorithm()
    print('Scores: %s' % scores)
    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))
  </code></pre>

</div> <!--end of main-->

{% endblock middle_column %}   
      
{% block right_column %}        
{% endblock right_column %}  
